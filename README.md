## Unlocking Employee Benefits: NLP-Powered Insights from Job Descriptions

### üìå Project Overview
- I showcase how to create an **end-to-end machine learning pipeline** to extract and categorise **employee benefits in a structured format** from unstructured job descriptions at scale.
- This enables **benefit insight generation** that would be useful for business HR functions, or individuals who are curious what type of benefits should be expected at their level! (fix)

To achieve this, I trained **two ML models**:
- **Named Entity Recognition (NER) Model** -> Extracts benefits from job descriptions along with contextual details e.g. "We offer health, dental and pet insurance"
- **Text Categorisation Model** -> Categorises extracted benefits into a list of predefined benefit categories

**Example Output:**

<table>
    <thead>
        <tr>
            <th>Job Description</th>
            <th>Extracted Benefit</th>
            <th>Categorised Benefit</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td rowspan="3">"We at ABC ltd are looking to hire an Insights Analyst, offering benefits such as a pension plan matching up to 5% of employer contribution, cycle2work scheme, health insurance, and much more!"</td>
            <td>pension plan matching up to 5% of employer contribution</td>
            <td>Retirement_benefits</td>
        </tr>
        <tr>
            <td>cycle2work scheme</td>
            <td>Transport_benefits</td>
        </tr>
        <tr>
            <td>risk insurance</td>
            <td>Insurance_benefits</td>
        </tr>
    </tbody>
</table>
<br>

### üéØ KPIs (Key Performance Indicators)
‚úÖ **Extraction Accuracy** - Ensure benefits are extracted with full context (e.g., pension contribution %)  
‚úÖ **Global Applicability** - Model is trained on job descriptions across **UK, US, Germany, India, Australia, France, and Canada**  
‚úÖ **High Recall Rate** ‚Äì Minimize overlooked benefits in job descriptions  
‚úÖ **False Positive Minimization** ‚Äì Avoid irrelevant extractions (e.g., distinguishing "health insurance" as a benefit vs. a job responsibility)  
<br>

### üìò Project notebooks
- üìÑ **[Main Workflow Notebook](/notebooks/Using_NER_and_Classificcation_models.ipynb)**

### üìÇ Dataset
**Source:** Job descriptions **scraped from online job boards** and stored in **Databricks Catalog**  
- **Raw Data Format:** HTML  
- **Preprocessing:** Converted to plain text, tokenized for NLP model training  
- **Annotations:** Manually labeled via **Prodi.gy**, with:  
  - **2,000+ annotated JDs for NER**  
  - **3,500+ annotated JDs for Text Categorization**  
<br>

### üîë Key steps
**1Ô∏è‚É£ Data Collection & Cleaning**  
- Extracted job descriptions from **Databricks data lake using SQL**  
- Cleaned text: removed HTML tags, tokenized sentences, lowercased words  

**2Ô∏è‚É£ Annotation & Model Training**  
- Created ground-truth, labeled dataset by annotating job descriptions manually using **Prodi.gy**
- Used a fine-tuned **BERT-based transformer models** for both NER & Categorisation

**3Ô∏è‚É£ Model Architecture**  
- **NER Model:** Uses **pre-trained BERT embeddings** + a custom NER component to detect **benefit phrases**  
- **Text Categorization Model:** **Multi-label classifier** that assigns benefits to **one or more categories** based on model confidence  

**4Ô∏è‚É£ Model Evaluation & Iteration**  
- Performance measured via **Precision, Recall, and F1 Score** 
- Logged model perforance on **ML Flow and Weightsandbiases** for documentation and ease of future usage 
- Iterative **retraining & tuning** until models met business accuracy thresholds 
<br>

### üöÄ Challenges & Insights 
‚ö†Ô∏è **Handling Class Imbalance**  
Some benefit categories were **underrepresented in the training data**. 
**Solution:**  
- **Used data augmentation techniques** to artificially balance the dataset.  
- **Optimized loss functions** to prevent bias towards overrepresented classes.  

‚ö†Ô∏è **Extracting Benefits with Context**  
Job descriptions often **mention benefits vaguely** (e.g., ‚ÄúGreat employee perks‚Äù).  
**Solution:** Custom rules to **ignore vague references** while preserving **relevant details** (e.g., pension contribution %).  

‚ö†Ô∏è **Avoiding False Positives**  
"Health Insurance Consultant" (job role) vs. "Health Insurance Provided" (benefit).  
**Solution:** Context-aware modeling using **Transformer-based embeddings** + custom **negative sampling during training**.  

<br>

### Notebooks
