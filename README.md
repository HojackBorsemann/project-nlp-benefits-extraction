## Unlocking Employee Benefits: NLP-Powered Insights from Job Descriptions

### ğŸ“Œ Project Overview
- I showcase how to create an **end-to-end machine learning pipeline** to extract and categorise **employee benefits in a structured format** from unstructured job descriptions at scale.
- This enables **benefit insight generation** that would be useful for business HR functions, or individuals who are curious what type of benefits should be expected at their level! (fix)

To achieve this, I trained **two ML models**:
- **Named Entity Recognition (NER) Model** -> Extracts benefits from job descriptions along with contextual details e.g. "We offer health, dental and pet insurance"
- **Text Classification Model** -> Categorises extracted benefits into a list of predefined benefit categories

**Example Output:**

<table>
    <thead>
        <tr>
            <th>Job Description</th>
            <th>Extracted Benefit</th>
            <th>Categorised Benefit</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td rowspan="3">"We at ABC ltd are looking to hire an Insights Analyst, offering benefits such as a pension plan matching up to 5% of employer contribution, cycle2work scheme, health insurance, and much more!"</td>
            <td>pension plan matching up to 5% of employer contribution</td>
            <td>Retirement_benefits</td>
        </tr>
        <tr>
            <td>cycle2work scheme</td>
            <td>Transport_benefits</td>
        </tr>
        <tr>
            <td>risk insurance</td>
            <td>Insurance_benefits</td>
        </tr>
    </tbody>
</table>
<br>

### ğŸ¯ KPIs (Key Performance Indicators)
âœ… **Extraction Accuracy** - Ensure benefits are extracted with full context (e.g., pension contribution %)  
âœ… **Global Applicability** - Model is trained on job descriptions across **UK, US, Germany, India, Australia, France, and Canada**  
âœ… **High Recall Rate** â€“ Minimize overlooked benefits in job descriptions  
âœ… **False Positive Minimization** â€“ Avoid irrelevant extractions (e.g., distinguishing "health insurance" as a benefit vs. a job responsibility)  
<br>

### ğŸ“˜ Project notebooks
- ğŸ“„ **[Using models to extract + classify benefits from job descriptions](/notebooks/Using_NER_and_Classificcation_models.ipynb)**
- ğŸ“„ **[Training NER model using spaCy](/notebooks/)**
- ğŸ“„ **[Training Text Classification model using spaCy](/notebooks/)**
- ğŸ“„ **[Extracting data using SQL](/notebooks/SQL_to_extract_data.ipynb)**

### ğŸ“‚ Dataset
**Source:** Job descriptions **scraped from online job boards** and stored in **Databricks Catalog**  
- **Raw Data Format:** HTML  
- **Preprocessing:** Converted to plain text, tokenized for NLP model training  
- **Annotations:** Manually labeled via **Prodi.gy**, with:  
  - **2,000+ annotated JDs for NER**  
  - **3,500+ annotated JDs for Text Classification**  
<br>

### ğŸ”‘ Key steps
**1ï¸âƒ£ Data Collection & Cleaning**  
- Extracted job descriptions from **Databricks data lake using SQL**  
- Cleaned text: removed HTML tags, tokenized sentences, lowercased words  

**2ï¸âƒ£ Annotation & Model Training**  
- Created ground-truth, labeled dataset by annotating job descriptions manually using **Prodi.gy**
- Used a fine-tuned **BERT-based transformer models** for both NER & Classification

**3ï¸âƒ£ Model Architecture**  
- **NER Model:** Uses **pre-trained BERT embeddings** + a custom NER component to detect **benefit phrases**  
- **Text Classification Model:** **Multi-label classifier** that assigns benefits to **one or more categories** based on model confidence  

**4ï¸âƒ£ Model Evaluation & Iteration**  
- Performance measured via **Precision, Recall, and F1 Score** 
- Logged model perforance on **ML Flow and Weightsandbiases** for documentation and ease of future usage 
- Iterative **retraining & tuning** until models met business accuracy thresholds 
<br>

### ğŸš€ Challenges & Insights 
âš ï¸ **Handling Class Imbalance**  
Some benefit categories were **underrepresented in the training data**. 
**Solution:**  
- **Used data augmentation techniques** to artificially balance the dataset.  
- **Optimized loss functions** to prevent bias towards overrepresented classes.  

âš ï¸ **Extracting Benefits with Context**  
Job descriptions often **mention benefits vaguely** (e.g., â€œGreat employee perksâ€).  
**Solution:** Custom rules to **ignore vague references** while preserving **relevant details** (e.g., pension contribution %).  

âš ï¸ **Avoiding False Positives**  
"Health Insurance Consultant" (job role) vs. "Health Insurance Provided" (benefit).  
**Solution:** Context-aware modeling using **Transformer-based embeddings** + custom **negative sampling during training**. 
